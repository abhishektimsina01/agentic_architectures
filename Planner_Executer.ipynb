{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5a0f49",
   "metadata": {},
   "source": [
    "## Planner-Executer architecture:\n",
    "\n",
    "It is a type of the arcitecture where the system first of all computes and thinks of a sequence of task/step to perform to achieve the goal before even starting to execute the first step. It gets ready for battle without starting it. It first of all makes a breif sequence of step the system has to follow to achieve goalm where the steps are the simple sub-goals that are made from the main complex goal.\n",
    "\n",
    "It is different from ReAct architecture as ReAct is for one step reasoning(step) as a time where as this is for plan once and reach the goal with the steps in the plan. ReAct is done for the goal that is lighter than the goal given to planner-executer as we dont have to reason more in ReaAct and ReAct is high bill generator as we have to keep reasoning as the goal gets complex which consumes high token eventually leading to high cost.\n",
    "\n",
    "### Components of planner architecture:\n",
    "\n",
    "It contains of two components:\n",
    "\n",
    "1. planner: the one that makes the plan by decomposing the goal into sub-gials that are simple and can be executed with 1 or 2 tool calls\n",
    "2. executor : the one that reads the plan step-by-step and executes the task and eventually reaches the goal.\n",
    "\n",
    "we still have the loop between the agent and the tools but the agent is executor that will execute the plan step by step\n",
    "\n",
    "\n",
    "If we have multiple executor agent based upon the task then we would distribute the task based upon the task\n",
    "Planner can be Multi-Agent system or may not be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "from typing import TypedDict, Annotated, List, Tuple, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from langchain_tavily import TavilySearch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a07747",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0250c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ChatGroq(\n",
    "    model=os.getenv(\"model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "model2 = ChatGroq(\n",
    "    model=os.getenv(\"model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "\n",
    "tavily_tool = TavilySearch(max_results=1, topic='general', include_answer=True)\n",
    "\n",
    "tavily_tool.invoke({'query' : \"What are the latest breakthroughs in AI for 2026?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanExecute(TypedDict):\n",
    "    goal : str\n",
    "    plan : Annotated[list[BaseMessage], add_messages]\n",
    "    answer : Annotated[list[AIMessage], add_messages] #this will contain the final answer\n",
    "    intermediate_steps : Annotated[ List[Tuple[Any, Any, Optional[Any]]], add_messages] #this will contain the intermediate plan answer like query, tool_calls, and ai messge\n",
    "\n",
    "class Output(BaseModel):\n",
    "    output : list[str] = Field(description=\"this should contain the list of the steps to follow to acheicve the goal\")\n",
    "\n",
    "output_format = PydanticOutputParser(pydantic_object=Output)\n",
    "\n",
    "model2.bind_tools([tavily_tool])\n",
    "\n",
    "tools = [tavily_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "tools_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8781d86",
   "metadata": {},
   "source": [
    "## Planner Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05321ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner(state: PlanExecute):\n",
    "    goal = state[\"goal\"]\n",
    "    template = PromptTemplate(template=\"\"\"For the given objective, come up with a simple step by step plan. The plan should contains only the steps or the question to be asked to the other llm and t should not contain the asnwer. \\\n",
    "    This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps. \n",
    "    goal: {goal} \\n\\n \n",
    "    the format of the plan made should be : {format_ins}\"\"\", input_variables=['goal'], partial_variables={\"format_ins\" : output_format.get_format_instructions()})\n",
    "    chain = template | model1 | output_format\n",
    "    output = chain.invoke(goal).output\n",
    "    print(\"the plan is\")\n",
    "    print(output)\n",
    "    return {'plan' : [output]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner(PlanExecute(goal=\"What is the population of the capital city of France? \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685239a1",
   "metadata": {},
   "source": [
    "## Executor Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executer(state : PlanExecute):\n",
    "    print(\"hello\")\n",
    "    plan = state['plan']\n",
    "    intermediate_step = state.get(\"intermediate_steps\")\n",
    "    print(intermediate_step)\n",
    "    if intermediate_step == None:\n",
    "        intermediate_step = []\n",
    "    current_plan = plan[0]\n",
    "    output = model2.invoke(f\"\"\"You are an Executor Agent.\n",
    "\n",
    "        You receive:\n",
    "        1. A task or objective\n",
    "        2. A finalized plan generated earlier\n",
    "        3. Any intermediate answers, decisions, or context from previous steps\n",
    "\n",
    "        Your responsibilities:\n",
    "        - Execute the task strictly according to the given plan.\n",
    "        - Use previous answers and context whenever required.\n",
    "        - Perform any necessary computation, tool usage, or synthesis.\n",
    "        - If execution is not required, directly provide the final answer.\n",
    "\n",
    "        Rules:\n",
    "        - Do NOT explain your reasoning.\n",
    "        - Do NOT show chain-of-thought, analysis, or internal decision-making.\n",
    "        - Do NOT restate the plan unless explicitly asked.\n",
    "        - Output ONLY the final result, solution, or answer.\n",
    "        - Be concise, correct, and execution-focused.\n",
    "                                \n",
    "\n",
    "                plan: {current_plan},\n",
    "                precious step: {intermediate_step}\n",
    "                                \n",
    "\n",
    "        If information is missing:\n",
    "        - Make the minimal reasonable assumption and proceed.\n",
    "        - Do NOT ask clarifying questions unless execution is impossible.\n",
    "\n",
    "        Output format:\n",
    "        - Final answer only.\n",
    "        - No markdown headings, no explanations, no meta-comments.\"\"\").content\n",
    "    intermediate_step.append((current_plan, output))\n",
    "    print(intermediate_step)\n",
    "    # keeps on executing the plans\n",
    "    plan = plan[1:]\n",
    "    return {'intermediate_steps' : [intermediate_step], 'plan' : [plan]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "executer(PlanExecute(plan=['What is the capital city of France?','What is the population of the city identified in the previous step?'], intermediate_steps=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_check(state : PlanExecute):\n",
    "    #  check if the AI messge we obtained containts the tool_calls or not, if not then end to the senthasizer or else call the corresponding tool call\n",
    "    if state['plan'] == []:\n",
    "        return 'synthesizer'\n",
    "    if state['answer'][-1].tool_calls != []:\n",
    "        return \"tool_node\"\n",
    "    else:\n",
    "        return \"execute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84eafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tool_node(state : PlanExecute):\n",
    "    # this will contian the logic to call the corresponding tool or tools asking by the tool_calls\n",
    "    # return to execute for the other plan to execute\n",
    "    answer = state['answer']\n",
    "    intermediate_step = state['intermediate_steps'][-1]\n",
    "    for tool in answer.tool_calls:\n",
    "        output = tools_by_name[tool.name].invoke(tool['args'])\n",
    "        intermediate_step.append(ToolMessage(content=output,  tool_call_id=tool[\"id\"]))\n",
    "    intermediate_steps = state['intermediate_steps']\n",
    "    intermediate_steps[-1] = intermediate_step\n",
    "    return {'intermediate_steps' : [intermediate_steps]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb22992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer(state: PlanExecute):\n",
    "    intermediate_step = state['intermediate_steps']\n",
    "    plan = state['plan']\n",
    "    answer = state['answer']\n",
    "    prompt = f\"\"\"Bssed upon the list of task and output with it in the for \n",
    "    [(task1, answe1, tool1), (task2, answe2, tool2), (task3, answe3, tool3), ....], aggregate the asnwer to one as the main goal is given:\n",
    "    plan and answers: {intermediate_step} \\n plan: {plan}\"\"\"\n",
    "    answer = model1.invoke(prompt).content\n",
    "    return {'answer' : answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373e0ee",
   "metadata": {},
   "source": [
    "### What happen inside the Plan-Execute workflow?\n",
    "\n",
    "##### The planner gets the goal from the user and then the planner with the help of the llm makes the llm call and plans the sequence of task to execute the goal or to reach goal. Then the plan is stored inside the state in the form of plan so that the executor can use it\n",
    "\n",
    "##### The plan is recieved by the executor and then executor with the llm + tools to execute the task and task only(not autonomous only free to use tool according to the task). The executer loops through every task and invoke it to llm binded with tools and recieves the reply from the llm, its either gonna be the content from llm itself or recommendation of tool mentioned in tool_calls. With every tool_calls it call the required tool and then recieves the outcome back saves it to the intermediate_step so that we can record the answer we have been getting from the execution.\n",
    "\n",
    "##### After execution of all the plan we again invoke llm but now for the enitre intermediate result we have been gettin till now to create a single answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71083d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(PlanExecute)\n",
    "\n",
    "graph.add_node(\"planner\", planner)\n",
    "graph.add_node(\"executor\", executer)\n",
    "graph.add_node(\"tool_node\", tool_node)\n",
    "graph.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "graph.add_edge(START, \"planner\")\n",
    "graph.add_edge(\"planner\", \"executor\")\n",
    "graph.add_conditional_edges(\"executor\", condition_check, {'executor' : 'executor', 'tool_node' : 'tool_node', 'synthesizer' : \"synthesizer\"})\n",
    "graph.add_edge(\"tool_node\", \"executor\")\n",
    "graph.add_edge(\"synthesizer\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.invoke({'goal' : \"What is the population of th country where Mt. Everest is situated at?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35578cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
