{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e61c19",
   "metadata": {},
   "source": [
    "## ReAct agent:\n",
    "It is a type of the agentic ai architecture where the agent performs the loop as:-\n",
    "\n",
    " (Reasoning ---> Action ---> Observe ---> Reasoning)\n",
    "\n",
    "Instead of making a plan(sequence of task) to achieve the end goal, in ReAct architecture, we do the reasoning step by step due to which we go from several loops over the brain and the tools depending upon the question and the question complexity.\n",
    "\n",
    "When the agent gets the question, \n",
    "\n",
    "\"what is the capital city of Nepal and what is the population of the capital city?\" then,\n",
    "It doesn't make the plan to reach the goal, what is does is it first reasons what is the capital city of Nepal and then after the result is accquired, it then finds the population of the result obtained from previous reasoning and in between it calls required tools and observes the answer from the tool.\n",
    "\n",
    "Like the other Agent, this Agent is binded with tools too and makes dynamic tool calls if the appropriate tool is avaiable like TavilySearch, bing, Arxiv, Wikipidea etc. \n",
    "\n",
    "This is the base of almost all agentic system and basic architecture for agentic system req uiring the single-agent.\n",
    "\n",
    "ReAct is less powerful and more costly than planner-executer architecture as we have to perform reasoning in the every step where as the planner-executer agent does it ones and executes it step-by-step as plan without invoking llm(brain) for next step which is more costly and high bills.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402469b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, TypedDict, Literal\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_tavily import TavilySearch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=os.getenv(\"model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state\n",
    "import operator\n",
    "class Messagestate(TypedDict):\n",
    "    messages : Annotated[list[BaseMessage], add_messages]\n",
    "    plan : str\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    plan : str = Field(description=\"the plan should in string format in point wide form\")\n",
    "\n",
    "PlanFormat = PydanticOutputParser(pydantic_object=Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_wallpaper = ArxivAPIWrapper(top_k_results= 1, doc_content_chars_max= 400)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_wallpaper)\n",
    "\n",
    "wikipedia_wallpaper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max= 150 )\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia_wallpaper)\n",
    "\n",
    "tavily_tool = TavilySearch(max_results=1, topic='general', include_answer=True)\n",
    "\n",
    "tavily_tool.invoke({'query' : \"What are the latest breakthroughs in AI for 2026?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e559c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wikipedia_tool.invoke(\"Criastiano Ronaldo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int)->int:\n",
    "    \"\"\"used to Multiply two given number `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"used to Adds two given number`a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"used to Subtract two given number a and b. \n",
    "    Args: \n",
    "    a: First int b: Second int\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"used to Divide two given number a and b.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "@tool\n",
    "def root_tool(a: float, b : float, c : float)-> str:\n",
    "    '''used to solve an quadratic quation given the 3 parametes, a, b and c where the eauipon is ax^2+bx+c=0 and return the value of x'''\n",
    "    discrimination =  b**2 - (4*a*c)\n",
    "    val = discrimination\n",
    "    if val < 0:\n",
    "        return \"There are imaginary root\"\n",
    "    elif val == 0:\n",
    "        root = (-b)/(2*a)\n",
    "        return f\"The root is {root} and {root}\"\n",
    "    else:\n",
    "        root1 = (-b + discrimination**0.5)/(2*a)\n",
    "        root2 = (-b - discrimination**0.5)/(2*a)\n",
    "        return {'result' : f\"The roots are : {root1} and {root2}\"}\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, subtract, multiply, divide, arxiv_tool, wikipedia_tool, tavily_tool, root_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(tools_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(state[\"messages\"])\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "# agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Show the agent\n",
    "# display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Who made Python language and multiply his age with 4\")]\n",
    "config = {'configurable' : {'thread_id' : 2}}\n",
    "messages = agent.invoke({\"messages\": messages}, config=config)\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e036e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = agent.get_state(config=config).values\n",
    "msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in msg['messages']:\n",
    "    if isinstance(m, HumanMessage):\n",
    "        print('Human')\n",
    "        print(m.content)\n",
    "    elif isinstance(m, AIMessage):\n",
    "        print('\\nAI')\n",
    "        # print(m.additional_kwargs['reasoning_content'])\n",
    "        print(m.content)\n",
    "        print(m.tool_calls)\n",
    "    else:\n",
    "        print(\"\\ntool\")\n",
    "        print(m)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a157fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in agent.get_state(config=config).values['messages']:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.get_state(config=config).values['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"give me the recent AI new and then multiply 48459594144 by 29854854824545\")]\n",
    "config = {'configurable' : {'thread_id' : 3}}\n",
    "messages = agent.invoke({\"messages\": messages}, config=config)\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae141289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
